{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d983d8b9",
   "metadata": {},
   "source": [
    "# About the notebook\n",
    "This notebook is designed to train a deep learning UNET architecture for segmenting and detecting structures in a 2D input image. The [UNET architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) is a popular choice for image segmentation tasks and it is composed of an encoder path, which extracts high-level features, and a decoder path, which takes the features extracted by the encoder path and generates.\n",
    "\n",
    "The notebook is intended to be user-friendly, intuitive and does not require any programming skills to train the model. The user only needs to provide a training set consisting of input images and their corresponding target masks (also called ground truth images). The input images represent the images that will be segmented, while the target images contain the desired segmentation mask for each input image.\n",
    "\n",
    "**Training dataset requirements:** To use the notebook, the user needs to provide the paths of the folders containing the input and target images. These folders should be organized in such a way that each input image has a corresponding target image with the same file name. For example, if the input image is named \"image_001.tif\", then the corresponding target image should be named \"image_001_target.tif\". **Please note that the current version of the notebook only accepts images in the TIF file format**. \n",
    "<br> *The input image* is required to be in a specific shape, which is \\[C1,H,W\\], where C1 is the number of channels, and H and W are the height and width of the image, respectively. This means that if the input image contains multiple channels, such as a RGB image, the channels should be included in a single file in the specified shape. \n",
    "<br> *The target image* should have the shape \\[C2,H,W\\], where C2 corresponds to the different objects to be segmented. The target image should be a binary image, where each object to be segmented is represented by a separate binary mask. The value of 1 in the mask corresponds to the object to be detected, and 0 corresponds to the background. It is important to note that the number of channels in the input image and the number of objects to segment in the target image may vary depending on the specific task and the dataset being used.\n",
    "\n",
    "Once the model is trained, the notebook \"Predict_Using_Model.ipynb\" can be used to generate segmentation masks for new input images. This can be done by providing a new set of input images and running the trained model on these images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415aac98",
   "metadata": {},
   "source": [
    "# 01 - Loading dependencies\n",
    "In this notebook, before running any code, there are several libraries and modules that need to be imported to ensure that the notebook runs smoothly. These libraries and modules contain pre-written code that performs specific tasks, such as reading and processing images, defining the UNET model, and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'workbookDir' not in globals():\n",
    "    print('Updating working directory')\n",
    "    workbookDir = os.path.dirname(os.getcwd())\n",
    "    os.chdir(workbookDir)\n",
    "print(os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "from core_code.deeplearning_util import get_model_outputdir\n",
    "from core_code.util import display_images_from_Dataset\n",
    "from core_code.UNet_2D.Dataset import CustomImageDataset\n",
    "from core_code.UNet_2D.UNet2d_model import Classic_U_Net_2D\n",
    "from core_code.train import train_model\n",
    "from core_code.parameters_interface.parameters_widget import parameters_model_training, parameters_folder_path ,parameters_training_images, parameters_device, parameters_data_augmentation\n",
    "from core_code.deeplearning_util import predict_model\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33824320",
   "metadata": {},
   "source": [
    "# 02 - Setting required paths of training set\n",
    "In this section, the user can specify the paths to the training set by specification the folders containing the input images and their corresponding target masks. This is done by setting two parameters: the \"Folder path input images\" and the \"Folder path target mask\".\n",
    "\n",
    "## Required parameters\n",
    "**Folder path input images**: the path of the folder containing the input images for the training set.\n",
    "\n",
    "**Folder path target mask**: the path of the folder containing the corresponding target masks for the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc4846",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters_training_set = parameters_training_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427da359",
   "metadata": {},
   "source": [
    "# 03 - Visualizing samples from training set\n",
    "This section is designed to help the user gain a better understanding of the input and target images in the training set. By visualizing these samples, the user can confirm that the images are properly paired and that the target masks accurately represent the structures to be detected in the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CustomImageDataset(parameters_training_set.get()[\"folder_input\"], parameters_training_set.get()[\"folder_target\"])\n",
    "print('Number of samples in training set: ' + str(len(training_dataset)))\n",
    "display_images_from_Dataset(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251778fb",
   "metadata": {},
   "source": [
    "# 04 - Data augmentation\n",
    "Data augmentation is a technique commonly used in machine learning to increase the size and variability of a training set by applying various transformations to the original data. These transformations may include flips, rotations, zooms, and shears, among others. The goal of data augmentation is to provide the model with additional training data that captures different variations of the same object, thereby improving the model's ability to generalize to new datasets.\n",
    "\n",
    "In cases where the training set is relatively small or contains limited variations, data augmentation can be especially useful. By increasing the variability of the training set, data augmentation helps to prevent the model from overfitting to the limited training data and producing inaccurate results on new datasets.\n",
    "\n",
    "In this notebook, users have the option to enable or disable data augmentation by selecting the appropriate flag. When disabled, the original training images are used without any transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e608bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_augmentation = parameters_data_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d9b68",
   "metadata": {},
   "source": [
    "## Visualizing samples of augmented images\n",
    "After enabling data augmentation in the notebook, users may want to visualize how the transformed images look like. The \"Display sample of augmented images\" section provides users with an option to visualize the augmented images in order to assess the effectiveness of the data augmentation process. By visualizing these images, users can determine if the data augmentation process is appropriate and if it is achieving the desired variability in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa40f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if parameters_augmentation.get()[\"data_augmentation_flag\"]:\n",
    "    training_dataset.set_data_augmentation(parameters_augmentation.get()[\"data_augmentation_flag\"], parameters_augmentation.get()[\"data_augmentation_object\"])\n",
    "    display_images_from_Dataset(training_dataset)\n",
    "else:\n",
    "    print('Data augmentation disable')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003cb966",
   "metadata": {},
   "source": [
    "# 05 - Constructing the U-Net model\n",
    "To create the U-Net model, the deep learning practitioner needs to define the architecture of the network, which involves specifying the layers and their connections. In this section, we construct the U-Net model\n",
    "\n",
    "## Select the device CPU or GPU (if available)\n",
    "The fact that the U-Net model consists of millions of parameters means that the optimization process can be computationally expensive and time-consuming. However, with the advent of specialized hardware, such as GPUs and TPUs, deep learning practitioners can accelerate the optimization process and train large models like U-Net in a reasonable amount of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63532188",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_device = parameters_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838952f4",
   "metadata": {},
   "source": [
    "## Construct the model in device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d35388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting data_augmentation in case it is required\n",
    "training_dataset.set_data_augmentation(parameters_augmentation.get()[\"data_augmentation_flag\"], parameters_augmentation.get()[\"data_augmentation_object\"])\n",
    "\n",
    "img, target  = training_dataset.__getitem__(0)\n",
    "n_channels_input  = img.shape[0]\n",
    "n_channels_target = target.shape[0] \n",
    "model = Classic_U_Net_2D(n_channels_input, n_channels_target)\n",
    "model.to(device= p_device.get_device())\n",
    "print('------------------------------')\n",
    "print('------------------------------')\n",
    "print('Creating a U-Net model. It receives images with ' + str(n_channels_input) + ' channels and predicts an image with ' + str(n_channels_target) + ' channels')\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('The U-Net model have ' + \"{:,}\".format(trainable_params) + ' parameters to optimize.')\n",
    "print('------------------------------')\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493bc9c",
   "metadata": {},
   "source": [
    "# 06 - Train the model\n",
    "The U-Net model consists of millions of parameters that need to be optimized to achieve accurate segmentation results.\n",
    "During the training process, the model learns to optimize its parameters to minimize a loss function, which measures the difference between the predicted segmentation and the ground truth segmentation. The optimization process requires defining several important parameters, including: \n",
    "<br>**batch size:** the number of images to be use per iteration to compute the gradient.  It is often recommended to use the largest batch size possible that can fit in the available memory without causing an out-of-memory error.\n",
    "<br>**number of epochs:**  This parameter specifies the number of times the entire dataset is processed during the training process. Setting an appropriate number of epochs  is important to ensure that the model converges to an optimal solution\n",
    "<br>**validation:** This parameter specifies a separate dataset that is used to evaluate the performance of the model during the training process. The validation dataset is typically used to monitor the progress of the model.\n",
    "<br>**loss function:** This parameter specifies the function used to measure the difference between the predicted outputs and the ground truth labels.\n",
    "<br>**optimizer:** This parameter specifies the optimization algorithm used to update the model parameters during training. \n",
    "\n",
    "\n",
    "## Setting parameters required to train the model\n",
    "You can set all the previously describe parameters using the following interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_model = parameters_model_training(model, training_dataset, n_channels_target = n_channels_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d1422",
   "metadata": {},
   "source": [
    "## Run training algorithm\n",
    "This function is the main routine for optimizing the parameters of the U-Net model, using the hyperparameters that are specified by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfa7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating data loaders\n",
    "train_loader = DataLoader(parameters_model.get(\"train_dataset\"), batch_size = parameters_model.get(\"batch\"), shuffle=True, drop_last=True)\n",
    "validation_loader = DataLoader(parameters_model.get(\"validation_dataset\"), batch_size = parameters_model.get(\"batch\"))\n",
    "\n",
    "#getting the criterion to be use to measure the performance of the model to predict target data\n",
    "loss_functions = parameters_model.get(\"loss_function\")\n",
    "\n",
    "# getting the optimizer to update weight\n",
    "optimizer = parameters_model.get(\"optimizer\")\n",
    "\n",
    "lr_scheduler = parameters_model.lr_scheduler.get(optimizer)\n",
    "\n",
    "# getting device to perform operations (recommended to use GPU)\n",
    "device = p_device.get_device()\n",
    "\n",
    "# number of iterations for each image in the training set\n",
    "epochs = parameters_model.get(\"epochs\")\n",
    "\n",
    "#parameters model_output\n",
    "output_dir = parameters_model.get(\"model_output_folder\")\n",
    "save_checkpoint = parameters_model.get(\"model_checkpoint\")\n",
    "checkpoint_frequency = parameters_model.get(\"model_checkpoint_frequency\")\n",
    "\n",
    "\n",
    "\n",
    "model = train_model(model = model,\n",
    "                    train_loader = train_loader,\n",
    "                    validation_loader = validation_loader,\n",
    "                    loss_functions = loss_functions,\n",
    "                    optimizer = optimizer,\n",
    "                    epochs = epochs,\n",
    "                    device = device,\n",
    "                    output_dir = output_dir,\n",
    "                    save_checkpoint = save_checkpoint,\n",
    "                    checkpoint_frequency = checkpoint_frequency,\n",
    "                    lr_scheduler = lr_scheduler\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb12513",
   "metadata": {},
   "source": [
    "# 07 [Optional] -  Evaluate performance of the model\n",
    "After training the model, it's important to evaluate its performance in terms of how well it can map input images to the desired ground-truth images. One way to measure performance is through the loss function, which should be reducing values as training progresses. This means that the model's error in predicting similar images to the ground truth is also reducing. If you have a validation set, the error on that set should also be decreasing over time.\n",
    "\n",
    "For a more detailed explanation of how to interpret training and validation loss in assessing model performance, you can refer to the following webpage: [Training and validation loss explanation](https://www.baeldung.com/cs/training-validation-loss-deep-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_tensorboard = Path(output_dir, 'tensorboard').as_posix()\n",
    "%tensorboard --logdir=$folder_tensorboard  --port=6006 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713f4a2",
   "metadata": {},
   "source": [
    "# 08 [Optional] - Testing the Trained Model\n",
    "After completing the training process, the trained model can now be used to predict masks for new and unseen data. To achieve this, we recommend using the Jupyter notebook **\"predict_using_trained_model.ipynb\"**.\n",
    "\n",
    "It's important to note that this section is only applicable if the model has been trained (step 06). As such, to avoid any confusion, we highly recommend using the provided Jupyter notebook to make predictions, as it doesn't require training a new model.\n",
    "\n",
    "you'll need to define the following parameters to test the trained model:\n",
    "<br>**Folder or file path**: Folder or file path containing the input images \n",
    "<br>**Folder output**: the folder where the model's output will be saved. If no folder output is specified, a folder named 'output' will be created in the folder containing the input images to save the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_test_set = parameters_folder_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d21ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_test_path, folder_output_test = parameters_test_set.get()\n",
    "predict_model(model, folder_test_path, folder_output_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
