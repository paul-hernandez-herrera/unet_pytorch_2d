{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d983d8b9",
   "metadata": {},
   "source": [
    "# About the notebook\n",
    "This notebook is designed to train a deep learning UNET architecture for segmenting and detecting structures in a 2D input image. The [UNET architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) is a popular choice for image segmentation tasks and it is composed of an encoder path, which extracts high-level features, and a decoder path, which takes the features extracted by the encoder path and generates.\n",
    "\n",
    "The notebook is intended to be user-friendly, intuitive and does not require any programming skills to train the model. The user only needs to provide a training set consisting of input images and their corresponding target masks (also called ground truth images). The input images represent the images that will be segmented, while the target images contain the desired segmentation mask for each input image.\n",
    "\n",
    "**Training dataset requirements:** To use the notebook, the user needs to provide the paths of the folders containing the input and target images. These folders should be organized in such a way that each input image has a corresponding target image with the same file name. For example, if the input image is named \"image_001.tif\", then the corresponding target image should be named \"image_001_target.tif\". **Please note that the current version of the notebook only accepts images in the TIF file format**. \n",
    "<br> *The input image* is required to be in a specific shape, which is \\[C1,H,W\\], where C1 is the number of channels, and H and W are the height and width of the image, respectively. This means that if the input image contains multiple channels, such as a RGB image, the channels should be included in a single file in the specified shape. \n",
    "<br> *The target image* should have the shape \\[C2,H,W\\], where C2 corresponds to the different objects to be segmented. The target image should be a binary image, where each object to be segmented is represented by a separate binary mask. The value of 1 in the mask corresponds to the object to be detected, and 0 corresponds to the background. It is important to note that the number of channels in the input image and the number of objects to segment in the target image may vary depending on the specific task and the dataset being used.\n",
    "\n",
    "Once the model is trained, the notebook \"Predict_Using_Model.ipynb\" can be used to generate segmentation masks for new input images. This can be done by providing a new set of input images and running the trained model on these images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55090e0e",
   "metadata": {},
   "source": [
    "# 00 - Special Instructions for Google Colab Users\n",
    "The following lines of code should be executed only when running your script on Google Colab. This is crucial to leverage the additional features provided by Colab, most notably, the availability of a free GPU. **If, you're running the code locally, this line can be skipped (GO TO STEP 01 - Loading dependencies) as it pertains specifically to the Colab setup.**\n",
    "\n",
    "# Give access to google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3514b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3603be9",
   "metadata": {},
   "source": [
    "# Install Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install napari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0524cf2",
   "metadata": {},
   "source": [
    "# Copy code to current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5eca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/paul-hernandez-herrera/unet_pytorch_2d\n",
    "import os\n",
    "workbookDir = \"/content/unet_pytorch_2d/\"\n",
    "os.chdir(workbookDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415aac98",
   "metadata": {},
   "source": [
    "# 01 - Loading dependencies\n",
    "In this notebook, before running any code, there are several libraries and modules that need to be imported to ensure that the notebook runs smoothly. These libraries and modules contain pre-written code that performs specific tasks, such as reading and processing images, defining the UNET model, and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'workbookDir' not in globals():\n",
    "    print('Updating working directory')\n",
    "    workbookDir = os.path.dirname(os.getcwd())\n",
    "    os.chdir(workbookDir)\n",
    "print(os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from core_code.util.deeplearning_util import get_model_outputdir, get_batch_size, get_model, load_model,calculate_test_performance,get_dataloader_file_names\n",
    "from core_code.util.util import write_list_to_file\n",
    "from core_code.util.show_image import show_images_from_Dataset\n",
    "from core_code.predict import predict_model\n",
    "from core_code.datasets.Dataset import CustomImageDataset\n",
    "from core_code.models.UNet2d_model import Classic_UNet_2D\n",
    "from core_code.train import train_model\n",
    "from core_code.parameters_interface.parameters_widget import parameters_model_training_segmentation, parameters_folder_path ,parameters_training_images, parameters_device, parameters_data_augmentation\n",
    "from core_code.parameters_interface.ipwidget_basic import set_Int\n",
    "from core_code.parameters_interface import options\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33824320",
   "metadata": {},
   "source": [
    "# 02 - Setting required paths of training set\n",
    "In this section, the user can specify the paths to the training set by specification the folders containing the input images and their corresponding target masks. This is done by setting two parameters: the \"Folder path input images\" and the \"Folder path target mask\".\n",
    "\n",
    "## Required parameters\n",
    "**Folder path input images**: the path of the folder containing the input images for the training set.\n",
    "\n",
    "**Folder path target mask**: the path of the folder containing the corresponding target masks for the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_training_set = parameters_training_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427da359",
   "metadata": {},
   "source": [
    "# 03 - Visualizing samples from training set\n",
    "This section is designed to help the user gain a better understanding of the input and target images in the training set. By visualizing these samples, the user can confirm that the images are properly paired and that the target masks accurately represent the structures to be detected in the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = CustomImageDataset(parameters_training_set.get()[\"folder_input\"], parameters_training_set.get()[\"folder_target\"])\n",
    "print('Number of samples in training set: ' + str(len(training_dataset)))\n",
    "show_images_from_Dataset(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251778fb",
   "metadata": {},
   "source": [
    "# 04 - Data augmentation\n",
    "Data augmentation is a technique commonly used in machine learning to increase the size and variability of a training set by applying various transformations to the original data. These transformations may include flips, rotations, zooms, and shears, among others. The goal of data augmentation is to provide the model with additional training data that captures different variations of the same object, thereby improving the model's ability to generalize to new datasets.\n",
    "\n",
    "In cases where the training set is relatively small or contains limited variations, data augmentation can be especially useful. By increasing the variability of the training set, data augmentation helps to prevent the model from overfitting to the limited training data and producing inaccurate results on new datasets.\n",
    "\n",
    "In this notebook, users have the option to enable or disable data augmentation by selecting the appropriate flag. When disabled, the original training images are used without any transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e608bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_augmentation = parameters_data_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d9b68",
   "metadata": {},
   "source": [
    "## Visualizing samples of augmented images\n",
    "After enabling data augmentation in the notebook, users may want to visualize how the transformed images look like. The \"Display sample of augmented images\" section provides users with an option to visualize the augmented images in order to assess the effectiveness of the data augmentation process. By visualizing these images, users can determine if the data augmentation process is appropriate and if it is achieving the desired variability in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa40f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if parameters_augmentation.get()[\"enable_data_augmentation\"]:\n",
    "    training_dataset.set_data_augmentation(parameters_augmentation.get()[\"enable_data_augmentation\"], \n",
    "                                           options.get_data_augmentation(parameters_augmentation.get()))\n",
    "    show_images_from_Dataset(training_dataset)\n",
    "else:\n",
    "    training_dataset.set_data_augmentation(parameters_augmentation.get()[\"enable_data_augmentation\"])\n",
    "    print('Data augmentation disable') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493bc9c",
   "metadata": {},
   "source": [
    "# 05 - Setting important parameters\n",
    "Deep learning models consists of millions of parameters that need to be optimized to achieve accurate segmentation results.\n",
    "During the training process, the model learns to optimize its parameters to minimize a loss function, which measures the difference between the predicted segmentation and the ground truth segmentation. The optimization process requires defining several important parameters, including: \n",
    "<br>**Device**: *Select the device CPU or GPU (if available)*. The fact that the U-Net model consists of millions of parameters means that the optimization process can be computationally expensive and time-consuming. However, with the advent of specialized hardware, such as GPUs and TPUs, deep learning practitioners can accelerate the optimization process and train large models like U-Net in a reasonable amount of time.\n",
    "<br>**Model output:** the folder path where the trained model will be stored for future use.\n",
    "<br>**number of epochs:**  This parameter specifies the number of times the entire dataset is processed during the training process. Setting an appropriate number of epochs  is important to ensure that the model converges to an optimal solution\n",
    "<br>**validation:** This parameter specifies a separate dataset that is used to evaluate the performance of the model during the training process. The validation dataset is typically used to monitor the progress of the model.\n",
    "<br>**loss function:** This parameter specifies the function used to measure the difference between the predicted outputs and the ground truth labels.\n",
    "<br>**optimizer:** This parameter specifies the optimization algorithm used to update the model parameters during training. \n",
    "\n",
    "\n",
    "## Setting parameters required to train the model\n",
    "You can set all the previously describe parameters using the following interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target  = training_dataset.__getitem__(0)\n",
    "n_channels_input  = img.shape[0]\n",
    "n_channels_target = target.shape[0]\n",
    "\n",
    "parameters_model = parameters_model_training_segmentation(n_channels_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe2a91",
   "metadata": {},
   "source": [
    "## Setting Batch size\n",
    "<br>**batch size:** the number of images to be use per iteration to compute the gradient.  It is often recommended to use the largest batch size possible that can fit in the available memory without causing an out-of-memory error.\n",
    "\n",
    "**We provide the following batch size for your computer specifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947392e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target  = training_dataset.__getitem__(0)\n",
    "batch_size = get_batch_size(model_type = 'unet_2d',\n",
    "                            device = parameters_model.get('device'),\n",
    "                            input_shape = (img.shape[0], img.shape[1],img.shape[2]),\n",
    "                            output_shape= (target.shape[0], target.shape[1], target.shape[2]),\n",
    "                            dataset_size= len(training_dataset),\n",
    "                            max_batch_size = 32\n",
    "    )\n",
    "print('------------------------------')\n",
    "print('\\033[41m' '\\033[1m' 'RECOMMENDED BATCH SIZE' '\\033[0m')\n",
    "print('You can set a lower value but not a higher one. A value equal to zero means that your device does not have enough memory for training, and you need to select another device.')\n",
    "print('------------------------------')\n",
    "batch_size_w = set_Int(\"batch_size:\", batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45a1e0",
   "metadata": {},
   "source": [
    "# 05 - Train the segmentation model\n",
    "This function serves as the primary routine for optimizing the parameters of the segmentation model. It utilizes the hyperparameters specified by the user to carry out the optimization process.\n",
    "\n",
    "## Run training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfa7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this parameters sets the number of models to generate\n",
    "NUM_RUNS = 1\n",
    "print('------------------------------')\n",
    "print('\\033[42m' '\\033[1m' 'PARAMETERS VALUES    ' '\\033[0m')\n",
    "print('------------------------------')\n",
    "p = parameters_model.get(\"device\")\n",
    "print(f\"Device: {p}\")\n",
    "p = parameters_model.get(\"model_output_folder\")\n",
    "print(f\"model_output_folder: {p}\")\n",
    "print(f\"image size: {img[0].shape}\")\n",
    "print(f\"batch_size: {batch_size_w.value}\")\n",
    "p = parameters_model.get(\"epochs\")\n",
    "print(f\"Epochs: {p}\")\n",
    "p = parameters_model.get(\"loss_function\")\n",
    "print(f\"loss_function: {p}\")\n",
    "p = parameters_augmentation.get()\n",
    "print(f\"data augmentation: {p}\")\n",
    "p = parameters_model.get(\"lr_scheduler_par\")\n",
    "print(f\"lr_scheduler_par: {p}\")\n",
    "p = parameters_model.get(\"validation_par\")\n",
    "print(f\"validation_par: {p}\")\n",
    "p = parameters_model.get(\"test_par\")\n",
    "print(f\"test_par: {p}\")\n",
    "print('------------------------------')\n",
    "print('------------------------------')\n",
    "\n",
    "for i in range(0,NUM_RUNS):\n",
    "    start = time.time()\n",
    "    \n",
    "    #getting batch size\n",
    "    batch_size = batch_size_w.value\n",
    "\n",
    "    # getting device to perform operations (recommended to use GPU)\n",
    "    device = parameters_model.get('device')\n",
    "\n",
    "    #construct the model\n",
    "    model_type = 'unet_2d'\n",
    "    model = get_model(model_type, n_channels_input, n_channels_target)\n",
    "    model.to(device= device)\n",
    "    print('------------------------------')\n",
    "    print(f\"Creating {model_type} model. It receives images with {n_channels_input} channels and predicts a segmentation with {n_channels_target} classes\")\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"The {model_type} model have {trainable_params:,} parameters to optimize.\")\n",
    "    print('------------------------------')\n",
    "    \n",
    "    #setting the values for data augmentation\n",
    "    data_augmentation_object = options.get_data_augmentation(parameters_augmentation.get())\n",
    "    training_dataset.set_data_augmentation(parameters_augmentation.get()[\"enable_data_augmentation\"], data_augmentation_object)    \n",
    "\n",
    "    #creating data loaders\n",
    "    new_training_dataset, validation_dataset, test_dataset = options.get_split_training_val_test_sets(\n",
    "        training_dataset,\n",
    "        parameters_model.get(\"validation_par\"), \n",
    "        parameters_model.get(\"test_par\"))\n",
    "\n",
    "    train_loader = DataLoader(new_training_dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size = batch_size) if validation_dataset else None\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size) if test_dataset else None\n",
    "\n",
    "    #getting the criterion to be use to measure the performance of the model to predict target data\n",
    "    loss_functions = options.get_loss_function(parameters_model.get(\"loss_function\"))\n",
    "\n",
    "    # getting the optimizer to update weight\n",
    "    optimizer = options.get_optimizer(option_name = parameters_model.get(\"optimizer_par\")[\"Optimizer\"],\n",
    "                                     model = model,\n",
    "                                     lr = parameters_model.get(\"optimizer_par\")[\"lr\"],\n",
    "                                     weight_decay = parameters_model.get(\"optimizer_par\")[\"weight_decay\"],\n",
    "                                     momentum = parameters_model.get(\"optimizer_par\")[\"momentum\"],\n",
    "                                     betas = parameters_model.get(\"optimizer_par\")[\"betas\"])\n",
    "\n",
    "    lr_scheduler = options.get_lr_scheduler(**parameters_model.get(\"lr_scheduler_par\"), optimizer = optimizer)\n",
    "\n",
    "    # number of iterations for each image in the training set\n",
    "    epochs = parameters_model.get(\"epochs\")\n",
    "\n",
    "    #parameters model_output\n",
    "    output_dir = parameters_model.get(\"model_output_folder\")\n",
    "    if NUM_RUNS>1:\n",
    "        output_dir = Path(output_dir, f\"RUN_{i}\")\n",
    "    save_checkpoint = parameters_model.get(\"model_checkpoint\")\n",
    "    checkpoint_frequency = parameters_model.get(\"model_checkpoint_frequency\")\n",
    "\n",
    "    model = train_model(model = model,\n",
    "                        train_loader = train_loader,\n",
    "                        validation_loader = validation_loader,\n",
    "                        loss_functions = loss_functions,\n",
    "                        optimizer = optimizer,\n",
    "                        epochs = epochs,\n",
    "                        device = device,\n",
    "                        output_dir = output_dir,\n",
    "                        save_checkpoint = save_checkpoint,\n",
    "                        checkpoint_frequency = checkpoint_frequency,\n",
    "                        lr_scheduler = lr_scheduler\n",
    "                       )\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"elapse time for training model {i}: {end - start} seconds\")\n",
    "    \n",
    "    write_list_to_file(Path(output_dir,'train_dataset.txt'), get_dataloader_file_names(train_loader)) \n",
    "    if test_dataset:\n",
    "        calculate_test_performance(model, test_loader, device, folder_output = output_dir)\n",
    "        write_list_to_file(Path(output_dir,'test_dataset.txt'), get_dataloader_file_names(test_loader))\n",
    "    \n",
    "    if validation_dataset:\n",
    "        write_list_to_file(Path(output_dir,'validation_dataset.txt'), get_dataloader_file_names(validation_loader))        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aadced",
   "metadata": {},
   "source": [
    "# 07 [Optional] -  Evaluate performance of the model\n",
    "After training the model, it's important to evaluate its performance in terms of how well it can map input images to the desired ground-truth images. One way to measure performance is through the loss function, which should be reducing values as training progresses. This means that the model's error in predicting similar images to the ground truth is also reducing. If you have a validation set, the error on that set should also be decreasing over time.\n",
    "\n",
    "For a more detailed explanation of how to interpret training and validation loss in assessing model performance, you can refer to the following webpage: [Training and validation loss explanation](https://www.baeldung.com/cs/training-validation-loss-deep-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_tensorboard = Path(output_dir, 'tensorboard').as_posix()\n",
    "%tensorboard --logdir=$folder_tensorboard  --port=6006 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713f4a2",
   "metadata": {},
   "source": [
    "# 08 [Optional] - Testing the Trained Model\n",
    "After completing the training process, the trained model can now be used to predict masks for new and unseen data. To achieve this, we recommend using the Jupyter notebook **\"predict_using_trained_model.ipynb\"**.\n",
    "\n",
    "It's important to note that this section is only applicable if the model has been trained (step 06). As such, to avoid any confusion, we highly recommend using the provided Jupyter notebook to make predictions, as it doesn't require training a new model.\n",
    "\n",
    "you'll need to define the following parameters to test the trained model:\n",
    "<br>**Folder or file path**: Folder or file path containing the input images \n",
    "<br>**Folder output**: the folder where the model's output will be saved. If no folder output is specified, a folder named 'output' will be created in the folder containing the input images to save the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_test_set = parameters_folder_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a59163",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_test_path, folder_output_test = parameters_test_set.get()\n",
    "predict_model(model, folder_test_path, folder_output_test, device = device);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
